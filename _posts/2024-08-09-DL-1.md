---
layout: post
title: Deep Learning practice 1
---



## **1-1. 퍼셉트론 작동 예시 구현하기**

이번 실습에서는 이론 영상을 통해 학습한 퍼셉트론의 작동 예시를 직접 코드로 구현해보도록 하겠습니다.

- 비의 유무와 여친의 만남 요청 유무 (**입력값** x1*x*1, x2*x*2)
- 비를 얼마나 좋아하는지와 여친을 얼마나 좋아하는지 (**가중치** w1*w*1, w2*w*2)
- 외출을 얼마나 좋아하는지 (**Bias** B*B*)

위 세 가지 사항과 아래의 표를 고려해서 외출 여부(**출력값** y*y*)를 판단하는 Perceptron 함수를 만들어봅시다.

![image](https://cdn-api.elice.io/api-attachment/attachment/4c02d3126c5f4b399510550b4503a6d5/perceptron.png)

x1*x*1, x2*x*2 , w1*w*1, w2*w*2값을 다양하게 입력하면 그에 따른 신호의 총합을 계산한 다음, 그 값을 활성화 함수가 전달받아 외출 여부를 나타내는 y*y*값 (**외출한다: 1, 외출하지 않는다: 0**)을 반환합니다.

예를 들어 x1*x*1 = 1, x2*x*2 = 0, w1*w*1 = -3, w2*w*2 = 4, B(bias)*B*(*bia**s*) = -1이라면 활성화 함수를 거친 값인 y*y*는 0이 되어 ‘외출하지 않는다’라는 결과를 얻을 것입니다.

## **실습**

1. 입력 받은 x1*x*1, x2*x*2, w1*w*1, w2*w*2 값들을 이용하여 계산한 신호의 총합 `output`과 그에 따른 외출 여부 `y`를 반환하는 `Perceptron` 함수를 완성하세요.
   - 활성화 함수는 ‘**신호의 총합이 0 초과면 외출하고, 0 이하라면 외출하지 않는다**‘는 규칙을 가집니다.
   - Bias 값은 외출을 좋아하는 정도를 의미하며, -1로 설정되어 있습니다.
2. 실행 버튼을 눌러 x1*x*1, x2*x*2, w1*w*1, w2*w*2 값을 다양하게 입력해보고, `Perceptron`함수에서 반환한 **신호의 총합**과 그에 따른 **외출 여부**를 확인해보세요.



## **1-2. DIY 퍼셉트론 만들기**

![image](https://cdn-api.elice.io/api-attachment/attachment/6094d9f6ab68480881ad93e500aad271/perceptron_node.png)

이번 실습에선 위 그림과 같은 퍼셉트론을 구현해 봅시다.

위 그림에서 **m = 4**로 설정하겠습니다. 따라서 **입력값**(**Input**)은 x1,x2,x3,x4*x*1,*x*2,*x*3,*x*4로 총 4개, **가중치**(**Weight**)도 w1,w2,w3,w4*w*1,*w*2,*w*3,*w*4로 총 4개입니다.

가중치 w0*w*0에 대한 입력값은 1이므로 두 값이 곱해진 값은 **상수** w0*w*0이고, 이는 곧 **Bias**입니다. 따라서 Bias b=1∗w0=w0*b*=1∗*w*0=*w*0 라고 할 수 있습니다.

입력값 x1,x2,x3,x4*x*1,*x*2,*x*3,*x*4와 가중치 w1,w2,w3,w4*w*1,*w*2,*w*3,*w*4, 그리고 1과 w0*w*0까지 Net input function에 입력되면 y=w1x1+w2x2+w3x3+w4x4+b*y*=*w*1*x*1+*w*2*x*2+*w*3*x*3+*w*4*x*4+*b*, 즉 **신호의 총합** 값이 나오게 됩니다.

y*y*는 이제 Activation function, 즉 활성화 함수에 입력값으로 들어가고, 우리의 퍼셉트론은 y*y*값에 따라 최종 신호 0 또는 1을 반환하게 됩니다.

## **실습**

1. 가중치 값이 들어간 1차원 리스트 `w`와 임의의 Bias 값인 `b`를 설정해줍니다.
2. 신호의 총합 `output`을 정의하고, `output`이 0 이상이면 1을, 그렇지 않으면 0을 반환하는 활성화 함수 `y`를 작성해 `perceptron` 함수를 완성합니다.



## 1-3. AND gate와 OR gate 구현하기

이번 실습에서는 이론 영상을 통해 학습한 퍼셉트론의 **AND gate**와 **OR gate**를 직접 구현해보도록 하겠습니다.

AND gate와 OR gate는 한 개의 퍼셉트론으로 만들어졌기 때문에 **단층 퍼셉트론**이라고 부릅니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/74fee3a24b4c405aa2e5291b7527a273/and_or.png)

단층 퍼셉트론인 AND gate와 OR gate를 직접 구현해보며 적절한 **가중치**(**Weight**)와 **Bias** 값을 찾아보고, 가장 기본적인 활성화 함수인 Step Function을 구현해보도록 하겠습니다.

**AND, OR gate 입출력 표**

| Input (x1) | Input (x2) | AND Output(y) | OR Output(y) |
| :--------- | :--------- | :------------ | :----------- |
| 0          | 0          | 0             | 0            |
| 0          | 1          | 0             | 1            |
| 1          | 0          | 0             | 1            |
| 1          | 1          | 1             | 1            |

## **실습**

1. `And_gate` 함수를 완성하세요.
2. `OR_gate` 함수를 완성하세요.
3. 활성화 함수인 `Step_Function` 함수를 아래 그래프를 참고하여 완성하세요.

![image](https://cdn-api.elice.io/api-attachment/attachment/dec5cb96d8274b699004b9feb54ba43b/step_function.png)

1. 실행 버튼을 눌러 입출력 표와 결괏값을 비교해본 후 제출하세요.

### Tips!

이전 실습과 달리 이번 실습은 입력값 `x`와 가중치 값 `weight`가 리스트가 아닌 **Numpy array** 형태로 주어져있습니다. 따라서 필요하다면 Numpy 연산 메소드를 사용해도 좋습니다.



## 1-4. NAND gate와 NOR gate 구현하기

앞선 실습에서는 가중치(Weight), Bias, Step Function을 이용하여 단층 퍼셉트론인 AND gate와 OR gate를 구현해보았습니다.

이번 실습에서는 가중치와 Bias 값을 조정해보며 동일한 단층 퍼셉트론 **NAND gate**와 **NOR gate**를 구현해보도록 하겠습니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/96c90279e980441ab498cbe7fbf03d75/sdjkfsjhdf.png)

**NAND, NOR gate 입출력 표**

| Input (x1) | Input (x2) | NAND Output(y) | NOR Output(y) |
| :--------- | :--------- | :------------- | :------------ |
| 0          | 0          | 1              | 1             |
| 0          | 1          | 1              | 0             |
| 1          | 0          | 1              | 0             |
| 1          | 1          | 0              | 0             |

## **실습**

1. 앞선 실습과 위의 입출력 표를 참고하여 `NAND_gate` 함수를 완성하세요. 이번 실습에서는 Bias 값 뿐만 아니라 가중치도 자유롭게 적절한 값을 설정해야합니다.
2. 앞선 실습과 위의 입출력 표를 참고하여 `NOR_gate` 함수를 완성하세요. 마찬가지로 Bias 값 뿐만 아니라 가중치도 자유롭게 적절한 값을 설정해야합니다.
3. 앞선 실습을 참고하여 `Step_Function` 함수를 완성하세요. 앞 실습에서 구현한 함수를 그대로 사용할 수 있습니다.
4. 실행 버튼을 눌러 입출력 표와 결괏값을 비교해본 후 제출하세요.



## **1-5. 비선형적인 문제 : XOR 문제**

**XOR 문제**는 주어진 두 x1, x2의 값이 다르면 결과가 1이 나오고 같으면 0이 나오는 문제 입니다. 이번 실습은 이진 분류가 가능한 선형 분류기 하나로 XOR 문제를 풀면 어떤 결과가 나오는지 확인하는 실습입니다.

**XOR gate 입출력 표**

| Input (x1) | Input (x2) | XOR Output(y) |
| :--------- | :--------- | :------------ |
| 0          | 0          | 0             |
| 0          | 1          | 1             |
| 1          | 0          | 1             |
| 1          | 1          | 0             |

XOR 문제의 경우는 지금까지 만든 AND, OR, NAND, NOR gate처럼 선형 분류기 하나로 문제를 해결할 수 없습니다. 즉 이는 **로지스틱 회귀**(**Logistic Regression**)로 문제를 해결할 수 없다는 뜻과 같습니다.

로지스틱 회귀는 **범주형 변수**를 회귀 예측하는 알고리즘을 말합니다. XOR gate를 포함한 AND, OR, NAND, NOR gate는 0과 1의 입력쌍을 통해 0 또는 1, 즉 두 **종류**의 변수를 예측합니다. 따라서 지금까지 배운 gate 알고리즘은 모두 로지스틱 회귀 알고리즘입니다.

------

아래의 그림은 분홍색 선인 선형 분류기가 XOR 문제를 풀 수 없음을 좌표 평면 상에서 나타낸 그림입니다.

![img](https://cdn-api.elice.io/api/archive/unzip_a2a6c193a33b47ddbd9c06c3289339e0/bb2571fd005978b14baca6410341425f76018d8b2d1119f21cb367c9c74c6583/XOR.png?se=2024-08-13T00%3A15%3A00Z&sp=r&sv=2021-12-02&sr=b&sig=7nSM43vqDDGwYF8crv8/zNk09UiWsrUZgSYx46Z6t/8%3D)

## **실습**

1. 앞선 실습과 위의 입출력 표를 참고하여 `XOR_gate` 함수를 최대한 완성해보세요. 이번 실습에서도 Bias 값 뿐만 아니라 가중치 또한 자유롭게 적절한 값을 설정해야합니다. 하지만 어떤 가중치와 Bias 값을 설정하더라도 **100% 완벽한 `XOR_gate` 함수 완성은 불가능**할 것입니다.
2. 앞선 실습을 참고하여 `Step_Function` 함수를 완성하세요. 앞 실습에서 구현한 함수를 그대로 사용할 수 있습니다.
3. 실행 버튼을 눌러 입출력 표와 결괏값을 비교해보고, 분류 정확도 `Accuracy`가 **50% 이상**이 되도록 만들어보세요.



## 1-6. 다층 퍼셉트론으로 XOR gate 구현하기

단층, 즉 한 개의 퍼셉트론으로 가중치와 Bias를 조정하여 구현한 AND, OR, NAND, NOR Gate는 하나의 직선으로 영역을 나눈 후 출력을 조정하여 나온 결과라고 할 수 있습니다.

**XOR Gate**를 완벽히 구현하기 위해선 어떻게 가중치와 Bias를 조정해야 할까요?

![image](https://cdn-api.elice.io/api-attachment/attachment/20ff4d6ec6a449c3a1cde161ef7d4964/%EB%B9%84%EC%84%A0%ED%98%95%EC%A0%81%EC%9D%B8%20%EB%AC%B8%EC%A0%9C.png)

위의 그림과 같이 한 개의 퍼셉트론으로는 하나의 직선으로 영역을 나누기 때문에 XOR Gate 구현이 불가능합니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/3719c0b887f342dcb55342b41d072b13/%EB%B9%84%EC%84%A0%ED%98%95%EC%A0%81%EC%9D%B8%20%EB%AC%B8%EC%A0%9C%20%EA%B5%AC%ED%98%84%EB%B0%A9%EB%B2%95.png)

하지만, 한 개의 퍼셉트론이 아닌 **여러 개**, 즉 **다층**으로 퍼셉트론을 쌓는다면 어떨까요? 다양한 퍼셉트론들을 활용하여 XOR Gate를 구현해보겠습니다.

**XOR gate 입출력 표**

| Input (x1) | Input (x2) | XOR Output(y) |
| :--------- | :--------- | :------------ |
| 0          | 0          | 0             |
| 0          | 1          | 1             |
| 1          | 0          | 1             |
| 1          | 1          | 0             |

## **실습**

1. `AND_gate` 함수를 확인하세요.
2. `OR_gate` 함수를 확인하세요.
3. `NAND_gate` 함수를 확인하세요.
4. `Step_Function` 함수를 확인하세요.

**5. 아래 그림을 참고하여 `AND_gate, OR_gate, NAND_gate` 함수들을 활용해 `XOR_gate` 함수를 구현하세요.**

![img](https://cdn-api.elice.io/api-attachment/attachment/490df63aec604a9ea2d96115c87a5131/image.png)

1. 실행 버튼을 눌러 입출력 표와 결괏값을 비교해본 후 제출하세요.



## **1-7. 다층 퍼셉트론(MLP) 모델로 2D 데이터 분류하기**

`data` 폴더에 위치한 `test.txt` 데이터와 `train.txt`데이터를 활용하여 2-D 평면에 0과 1로 이루어진 점들의 각 좌표 정보 (x,y)(*x*,*y*) 를 통해 모델을 학습시키겠습니다.

`hidden_layer_sizes`를 조정해보면서 다층 퍼셉트론 분류 모델을 학습시켜 모델의 정확도가 **81%** 를 넘도록 해봅시다.

이전 실습에서 단층 퍼셉트론인 AND, OR, NAND Gate를 구현하고, 이들을 쌓아서 XOR Gate를 만들어 보았습니다. 즉, XOR Gate는 단층 퍼셉트론을 여러 개 쌓아서 만든 **다층 퍼셉트론** (**MLP**: Multi Layer Perceptron)이라고 할 수 있습니다. 다층 퍼셉트론으로 만든 모델은 **인공 신경망**이라고도 합니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/4bf24b76620e4087b44071829643ca05/image.png)

이번 실습에서는 사이킷런에서 제공하는 다층 퍼셉트론 모델인 `MLPClassifier`를 이용해서 **2D 데이터**를 분류해보겠습니다.

------

**다층 퍼셉트론 모델을 사용하기 위한 사이킷런 함수/라이브러리**

- `from sklearn.neural_network import MLPClassifier`: 사이킷런에 구현되어 있는 다층 퍼셉트론 모델을 불러옵니다.
- `MLPClassifier(hidden_layer_sizes)`: MLPClassifier를 정의합니다.
- hidden_layer_sizes: 간단하게 hidden layer의 크기를 조절할 수 있는 인자입니다.

------

`MLPClassifier`는 역전파(backpropagation)라는 기법을 사용하여 모델을 학습합니다. 역전파는 2장에서 자세하게 배울 수 있으니 여기선 용어만 알아둡시다.

밑의 소스 코드는 첫 번째 히든층에 4개, 두 번째 히든층에 4개의 퍼셉트론을 가지게 설정한 것이고, 위 그림과 같은 모델을 나타냅니다.

```
clf = MLPClassifier(hidden_layer_sizes=(4, 4))

clf.fit(X, Y)
Copy
```

## **실습**

1. `MLPClassifier`를 정의하고 `hidden_layer_sizes`를 조정해 히든층의 레이어의 개수와 퍼셉트론의 개수를 바꿔본 후, 학습을 시킵니다.
2. 정확도를 출력하는 `report_clf_stats`함수를 완성합니다. `hit`는 맞춘 데이터의 개수, `miss`는 못 맞춘 데이터의 개수입니다. 정확도 점수 `score`는 **(맞춘 데이터의 개수 / 전체 데이터 개수) x 100**으로 정의하겠습니다. score를 코드로 작성해보세요.
3. 앞서 완성한 함수를 실행시키는 `main` 함수를 완성합니다. 코드 주석의 Step들을 참고하세요.
4. 일반적으로, 레이어 내의 퍼셉트론의 개수가 많아질수록 성능이 올라가는 것을 확인할 수 있습니다. 레이어의 개수와 퍼셉트론의 개수를 극단적으로 늘려보기도 하고, 줄여보면서 정확도를 **81%** 이상으로 늘려보세요.

### **출력 예시**

아래 이미지는 학습 결과에 대한 출력 예시입니다. 그래프에서 나타나는 배경색은 모델이 예측한 결과, O/X 가 나타내는 색은 실제값 (ground truth) 입니다. 모델이 맞게 예측한 경우 O, 틀리게 예측한 경우 X 로 표시됩니다.

![image](https://cdn-api.elice.io/api-attachment/attachment/4ead61a8c13748dab3f6a93f7a9d20bd/image.png)



## 1-8. 손글씨 분류하기

이번 실습에서는 다층 퍼셉트론 모델을 이용해 **손글씨 분류**를 해보겠습니다. 손글씨 데이터는 사이킷런에 이미 준비되어 있는 데이터를 사용합니다.

0부터 9까지의 숫자에 대한 10개 클래스의 글씨 데이터이며, 총 1797개의 손글씨 데이터가 있습니다. 각각의 손글씨 데이터는 8 x 8 격자(픽셀)로 이루어져 있습니다.

![MNIST](https://cdn-api.elice.io/api-attachment/attachment/32528dbee33d413a9101a98f5fe6a7ce/image.png)

손글씨 데이터를 분류하기 위해 앞선 실습과 마찬가지로 이번에도 사이킷런에서 제공하는 다층 퍼셉트론 모델 `MLPClassifier`을 사용하겠습니다. 다층 퍼셉트론 모델을 사용하기 위한 사이킷런 함수/라이브러리는 앞선 실습의 설명을 참고하세요.

이 외 다양한 파라미터는 [이 링크](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)에서 확인하실 수 있습니다.

## **실습**

`hidden_layer_sizes`를 조정해보면서 다층 퍼셉트론 분류 모델을 학습시켜 모델의 정확도가 **95%** 이상이 되도록 해봅시다.

1. 손글씨 데이터를 `X`, `y`로 읽어온 후 앞의 1600개는 학습 데이터, 나머지는 테스트 데이터로 나눕니다.
2. 이전 실습과 마찬가지로 `MLPClassifier`를 정의하고 `hidden_layer_sizes`를 조정해 hidden layer의 크기 및 레이어의 개수를 바꿔본 후, 학습을 시킵니다. 이때, `solver='adam'`, `beta_1=0.999999`로 설정합니다.
3. 정확도를 출력하는 `report_clf_stats`함수를 완성합니다. 이전 실습을 참고하세요.
4. 앞서 완성한 함수를 실행시키는 `main` 함수를 완성합니다. 코드 주석의 Step들을 참고하세요.

이전 실습처럼 레이어의 수와 퍼셉트론의 개수를 조절해보면서 정확도를 **95% 이상**으로 늘려보세요.



## 1-9. 퍼셉트론 선형 분류기를 이용해 붓꽃 데이터 분류하기

이번 실습에서는 퍼셉트론 선형 분류기를 이용하여 **iris data**(붓꽃 데이터)를 분류해보도록 하겠습니다.

iris data는 사이킷런에 이미 준비되어 있는 데이터이며, 꽃의 정보를 저장하고 있는 data와 해당 꽃의 품종을 저장하는 target으로 이루어져 있습니다.

------

data는 다음과 같은 **4가지의 feature**로 이루어져 있으며, (150, 4) shape을 가진 numpy array입니다.

- sepal length(꽃받침 길이)
- sepal width (꽃받침 넓이)
- petal length (꽃잎 길이)
- petal width (꽃잎 넓이)

target은 다음과 같은 **세 종류**의 붓꽃 품종이 저장되어 있으며, (150, ) shape을 가진 numpy array입니다.

- setosa
- versicolor
- virginica

------

이번 실습에서는 사이킷런에서 제공하는 퍼셉트론 선형 분류기 `Perceptron`과 붓꽃의 ‘petal length(꽃잎 길이)’, ‘petal width(꽃잎 넓이)’ **2가지 feature**들을 이용하여 붓꽃 품종을 분류해봅니다.

그 다음, 모델의 파라미터 값을 조정하여 테스트 데이터에 대한 분류 정확도를 **90% 이상**으로 높여보겠습니다.

------

**퍼셉트론을 구현하기 위한 사이킷런 함수/라이브러리**

- `from sklearn.linear_model import Perceptron`: 사이킷런에 구현되어 있는 Perceptron 모델을 불러옵니다.
- `Perceptron(max_iter, eta0)`
  : Perceptron 모델을 정의합니다.
  - max_iter : 모델 학습 최대 횟수
  - eta0 : 모델에 사용되는 가중치가 업데이트 되는 속도 (default = 1)

## **실습**

1. iris 데이터를 불러오고, 불러온 데이터를 학습용 데이터 80%, 테스트용 데이터 20%로 분리하여 반환하는 함수를 구현합니다.
2. 사이킷런의 Perceptron 클래스를 사용하여 Perceptron 모델을 정의한 후, 학습용 데이터에 대해 학습시키고 테스트 데이터에 대해 예측을 합니다.
3. 실행 버튼을 눌러 분류 정확도를 확인해보고, 정확도를 **90% 이상** 으로 높일 수 있도록 `Perceptron`의 파라미터를 수정해 제출하세요.