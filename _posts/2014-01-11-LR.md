---
layout: post
title:  "로지스틱 회귀(Ligistic Regression)란?"
---


### 이진 분류(Binary Classification)

- **이진 분류란, 문제에 대한 정답을 두 가지 답 중 하나로 분류하는 것을 의미한다.**

  - 예를 들어, 문제에 대한 정답이 0 과 1 중 하나라면,
  - 해당 문제에 대한 정답이 1 일 확률이 출력되고,
  - 해당 확률이 0.5 이상이면 1 로 판단한다.

- **그렇다면, 이진 분류 문제는 선형 회귀(Linear Regression)로 해결 할 수 있을까?**

  - **선형 회귀는 아래 그림과 같이, Outlier(이상치)에 약하기 때문에 분류 문제에 잘 동작하지 않는다.**

    

    ![img](https://blog.kakaocdn.net/dn/sajVc/btrmFNxTX7z/p3PrQD6cRbwotST5GR57Qk/img.png)

    

- **따라서, 이러한 이진 분류 문제를 해결하기 위한 회귀 방법 중 하나가, 로지스틱 회귀(Logistic Regression)이다.**

### 로지스틱 회귀(Logistic Regression)

- **가설(Hypothesis)**

  - 선형 회귀(Linear Regression)에서는 가설(H(x))로 직선(Wx+b)을 사용한다.
  - 하지만, 직선은 이상치를 분류하지 못한다.
  - 또한, 우리가 원하는 결과값은 0 과 1 사이의 값이지만, 직선으로 예측값을 추출하면, 보통 0 보다 작거나 1 보다 크기 때문이다.
  - **따라서, 로지스틱 회귀(Logistic Regression) 에서는, 가설로 시그모이드(sigmoid) 함수를 사용한다.**

- **시그모이드(Sigmoid) 함수**

  

  ![img](https://blog.kakaocdn.net/dn/qlDRK/btrmGg7Qebo/OTzQ5WE2IjulVGD50F4RC0/img.png)

  

  - 시그모이드 함수를 사용한다면,
  - x의 값이 작을 때의 예측값은 1/(1+무한대) 으로 0 에 수렴할 것이며,
  - x의 값이 클 때의 예측값은 1/(1+0) 으로 1 에 수렴할 것이다.
  - 따라서, 예측 값을 0 과 1 사이의 값으로 추출할 수 있게 된다.

- **비용 함수(cost function)**

  - 선형회귀(Linear Regression)에서는 직선을 가설로 사용하고, mse(mean square error)를 비용 함수로 사용한다.
  - 평균 제곱 오차는 실제 값과 예측 값 간의 차이를 제곱한 후, 그 평균을 계산한 것입니다. 주로 회귀 문제에서 모델의 성능을 측정하는 데 사용한다.
    <img width="263" alt="image" src="https://github.com/leelang7/leelang7.github.io/assets/67309088/af8922f2-6c5f-42cc-9a8a-01a993ac587e">

    * RMSE(Root Mean Squared Error) : 평균 제곱근 편차를 의미합니다. RMSE는 MSE의 제곱근으로, 모델의 예측 오차를 평가하는 지표. RMSE는 특히 예측 값의 오차의 단위를 원래의 단위로 되돌리기 위해 사용
<img width="206" alt="image" src="https://github.com/leelang7/leelang7.github.io/assets/67309088/aba0f435-ce33-49a0-89f1-4e86b51d93be">

  - 하지만, 로지스틱 회귀(Logistic Regression)에서는 가설로 시그모이드(Sigmoid) 함수를 사용하기 때문에, mse 로는 비용 함수를 구하기 어렵다.

  - **만약, 가설로 시그모이드(Sigmoid) 함수를 사용하고, 비용 함수로 mse 를 사용한다면 아래와 같은 결과가 나타날 것이다.**

    

    ![img](https://blog.kakaocdn.net/dn/vH3aD/btrmALnXY8r/AXGot9tvzxyhJ1UfisBWuK/img.png)

    

  - 위 그림처럼, 평야가 많아지기 때문에, 아무리 학습을 많이해도, mse 를 사용해 구한 cost 함수에서는 gradient descent 알고리즘이 제대로 동작하지 않게 된다.

  - **이를 해결하기 위해, Logistic Regression 에서는 비용함수로 Binary Cross Entropy 를 사용한다.**

- **Binary Cross Entropy**

  - **H(x) 는 시그모이드 함수로부터 나온 0 과 1 사이의 노드 예측값이다.**

  - **Binary Cross Entropy 는 cost 를 계산할 때, 노드의 실제 값에 따라 다른 방식으로 오차를 구한다.**

    

    ![img](https://blog.kakaocdn.net/dn/cnpFl8/btrmCOq3Vta/MgkJKWpWWA9GfDwDu62pv0/img.png)

    

    - **n 은 분류 노드 개수를 의미한다.**
    - **노드가 1개 일 땐, 단일 이진 분류**
    - **노드가 여러 개 일 땐, 멀티 이진 분류이다.**

  - **아래 그림은 실제 값과 예측값에 따른 비용함수이다.**

    

    ![img](https://blog.kakaocdn.net/dn/bj5PtG/btrmxiUc47u/KKPiRs1LhBGVKV4AHFjZq1/img.png)

    

    - 실제 값(y)이 1 일 때, 손실은 -log(H(x)) 로 계산된다.
      - -log(H(x))가 작아지기 위해선, log(H(x))값이 커져야하기 때문에, H(x) 값이 커져야한다.
        - H(x)가 1일 때 log(H(x))는 0이며, 0일 때 log(H(x))는 -무한대이기 때문이다.
      - 따라서, 실제 값(y)이 1일 땐, H(x)가 1이 되도록 진행된다.
    - 실제 값(y)이 0 일 때, 손실은 -log(1-H(x)) 로 계산된다.
      - -log(1-H(x))가 작아지기 위해선, log(1-H(x))값이 커져야하기 때문에, H(x) 값이 작아져야한다.
        - H(x)가 1일 때 log(1-H(x))는 -무한대이며, 0일 때 log(1-H(x))는 0이기 때문이다.
      - 따라서, 실제 값(y)이 0일 땐, H(x)가 0이 되도록 진행된다.

### 결론

- Linear Regression
  - 가설 : 직선
  - cost 함수 : Mean Square Error
- Logistic Regression
  - 가설 : Sigmoid
  - cost 함수 : Binary Cross Entropy

Source : https://wooono.tistory.com/115
