---
layout: post
title: Deep Learning practice 2
---



## 1-1. Gradient descent 알고리즘 구현하기

**Gradient descent** 알고리즘은 **손실 함수(loss function)의 미분값**인 **gradient**를 이용해 모델에게 맞는 **최적의 가중치**(**weight**), 즉 손실 함수의 값을 최소화 하는 가중치를 구할 수 있는 알고리즘입니다.

이번 실습에서는 Gradient descent 알고리즘을 직접 구현한 후, 이를 이용해 데이터를 가장 잘 설명하는 **선형 회귀 직선의 기울기와 y절편**, 즉 선형 회귀 모델에게 맞는 최적의 가중치를 찾아보겠습니다.

선형 회귀 직선의 수식은 다음과 같은 1차 함수 형태이며, 우리가 Gradient descent 알고리즘을 사용해 찾을 값, 즉 가중치는 w0과 w1입니다.

f(x)=w0+w1x*f*(*x*)=*w*0+*w*1*x*

------

### **손실 함수 (loss function)**

손실 함수(loss function)는 **실제값과 모델이 예측한 값 간의 차이**를 계산해주는 함수입니다. 손실 함수의 값은 가중치와 편향을 업데이트하는 데에 사용됩니다. 여기서는 손실 함수로 **MSE** (Mean Squared Error)를 사용합니다.

MSE는 **평균 제곱 오차** 함수로, 수식은 다음과 같습니다.

![image-20240812203506456](C:\Users\admin\Documents\이석창\마크다운\assets\image-20240812203506456.png)

![image-20240812203526111](C:\Users\admin\Documents\이석창\마크다운\assets\image-20240812203526111.png)

------

### **편미분**

gradient에는 **편미분**이라는 개념이 들어갑니다. 때문에 gradient를 설명하기 전 편미분에 대해 간단하게 짚고 넘어가겠습니다. 편미분이란 2개 이상의 변수를 가진 함수에서 우리가 미분할 하나의 변수를 제외한 나머지 변수들을 상수로 보고, 미분 대상인 그 변수로 미분하는 것입니다.

![image-20240812203553189](C:\Users\admin\Documents\이석창\마크다운\assets\image-20240812203553189.png)

------

### **Gradient**

gradient는 곧 **기울기 벡터**를 의미하며, 선형 함수의 각 파라미터들의 편미분으로 구성된 열벡터로 정의합니다.

강의 자료 9페이지를 보면 학습률(learning rate)을 나타내는 α*α*가 있고, gradient를 나타내는 수식인 ▽Loss(W)▽*L**oss*(*W*)가 있습니다. 즉 이를 풀어서 쓰면 다음과 같은 열벡터 형태입니다.

![image-20240812203620595](C:\Users\admin\Documents\이석창\마크다운\assets\image-20240812203620595.png)

![image-20240812203649754](C:\Users\admin\Documents\이석창\마크다운\assets\image-20240812203649754.png)



## **실습**

1. 설명 중 ‘손실 함수’ 파트의 수식을 참고해 MSE 손실 함수를 완성하세요.
2. 설명 중 ‘Gradient’ 파트의 마지막 두 수식을 참고해 `w0`와 `w1`에 대한 gradient인 `gradient0`과 `gradient1`을 반환하는 함수를 완성하세요.
3. 설명 중 ‘가중치 업데이트’ 파트의 두 수식을 참고해 gradient descent를 통한 가중치 업데이트 코드를 작성하세요.



## 1-2. 역전파(Back propagation)

**역전파**(**Back propagation**)는 다층 퍼셉트론 모델을 이루는 가중치들을 개선하기 위해 개발된 여러 알고리즘들 중 가장 유명하고 널리 쓰이는 방법입니다.

이번 실습에서는 역전파를 간단하게 실습해보기 위해, 퍼셉트론 한 개의 가중치들을 개선시키는 역전파를 구현해 보도록 합니다.

------

다음 그림은 이번 실습에서 사용되는 퍼셉트론을 나타냅니다. 입력은 x1,x2,x3*x*1,*x*2,*x*3 세 개의 정수로 주어지고, 각각 w1,w2,w3*w*1,*w*2,*w*3의 계수가 곱해져 **sigmoid 함수를 통과할 값**은 x1w1+x2w2+x3w3*x*1*w*1+*x*2*w*2+*x*3*w*3가 됩니다.

여기서 w1,w2,w3*w*1,*w*2,*w*3가 바로 우리가 이번 실습에서 알아내야 하는 가중치입니다.

![image1](https://cdn-api.elice.io/api/archive/unzip_e78be103c8554fe6b34d0493689412d3/4ee84a82b5e4c9e6651b13fd27dcf615e427ec584929f2cef7167aa99151a77a/image.png?se=2024-08-15T00%3A15%3A00Z&sp=r&sv=2021-12-02&sr=b&sig=zMrNB3DD61jcxjQ7khtv1j7LCcuQu%2BRIMcJE5Fd28wA%3D)

x1w1+x2w2+x3w3*x*1*w*1+*x*2*w*2+*x*3*w*3가 sigmoid 함수를 거치고 나면 0 ~ 1 사이의 값으로 변환됩니다. 이는 특정 클래스로 분류될 확률을 나타내며, 0.5보다 작을 경우 0으로, 0.5보다 클 경우 1로 분류된다고 합시다.

------

이제 이 퍼셉트론을 학습시키려고 합니다. 좀 더 정확히 이야기하면, x1,x2,x3*x*1,*x*2,*x*3와 그 클래스 y*y*가 여러 개 주어질 때, y*y*값을 가장 잘 예측하는 w1,w2,w3*w*1,*w*2,*w*3를 찾아야 합니다.

예를 들어, 우리가 갖고 있는 훈련용 데이터가 다음과 같이 3개로 주어진다고 합시다.

- (1, 0, 0) –> 0
- (1, 0, 1) –> 1
- (0, 0, 1) –> 1

그렇다면 w1*w*1 = 0, w2*w*2 = 0, w3*w*3 = 1 이어야 함을 알 수 있습니다.

------

물론 이와 같은 최적의 w1*w*1, w2*w*2, w3*w*3 값을 처음부터 알 수는 없습니다. 따라서 우선 **가중치 w\*w\*들을 초기화하고**, 이를 여러 번의 학습을 거쳐 알아내야 합니다.

즉, 손실 함수(loss function)의 gradient 값을 역전파해서 받은 후, 그 값을 참고하여 손실 함수값을 최소화 하는 방향으로 w1,w2,w3*w*1,*w*2,*w*3를 **업데이트** 합니다.

이때, w1,w2,w3*w*1,*w*2,*w*3이 잘 개선되서 **더 업데이트해도 변화가 거의 없을 때까지** 하는 것이 중요합니다.

## **실습**

코드의 주석 설명을 따라서 `getParameters(X, y)` 함수를 완성하세요. 여기서 `X`와 `y`는 훈련용 데이터입니다. 필요하다면 설명의 **굵은 글씨 부분**을 참고하세요.

1. `X`의 한 원소가 3개이므로 가중치도 3개가 있어야 합니다. 초기 가중치 `w`를 [1,1,1]로 정의하는 코드를 작성하세요.
2. 초기 가중치 `w`를 모델에 맞게 계속 업데이트 해야합니다. 업데이트를 위해 초기 가중치 `w`에 더해지는 값들의 리스트 `wPrime`을 [0,0,0]로 정의하는 코드를 작성하세요.
3. sigmoid 함수를 통과할 `r`값과 sigmoid 함수를 통과한 `r`값인 `v`를 정의하세요.
4. 가중치 `w`가 더이상 업데이트가 안될 때까지 업데이트해주는 코드를 작성하세요. 자세한 내용은 코드 주석을 참고하세요.

더 나아가 여러 예제를 테스트 해 보면서 하나의 퍼셉트론이 100% 예측할 수 없는 훈련용 데이터는 어떤 것이 있는지 생각해보세요.

이는 1장에서 배웠듯이 왜 다층 퍼셉트론 모델이 데이터 분류에서 필요한지 알게 해줍니다.