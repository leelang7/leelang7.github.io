---
layout: post
title: "Ridge와 Lasso" 
---

## 🟡 라쏘 회귀란?

→ 라쏘 회귀는 **선형 회귀의 또 다른 규제된 버전**

릿지 회귀처럼 비용 함수에 규제항을 더하지만 릿지 회귀와 다르게 가중치 벡터 *l*1 노름을 사용한다.

**🟨 라쏘가 등장한 이유**

: 릿지 회귀는 모든 p개의 변수를 포함하고 패널티 항은 모든 계수가 0이 되는 방향으로 수축한다. 람다가 무한이 아닌 경우에 계수를 정확히 수축하지 않는다. `모델의 정확도 관점에서는 문제가 되지 않지만 모델을 해석해야 하는 경우 문제점이 발생할 수 있기 때문에 Lasso 가 등장`하였다.

**라쏘 회귀의 비용 함수**

*J*(*θ*)=*M**S**E*(*θ*)+*α**i*=1∑*n*∣*θ**i*∣

-라쏘 회귀의 중요한 특징은 **`덜 중요한 특성의 가중치를 제거`**하려고 하는 것 **( 가중치가 0 이 된다는 뜻 !)**

![img](https://velog.velcdn.com/images/jochedda/post/ca647d20-c72e-47d4-b0c1-97012a3b9647/image.png)

→ 라쏘 규제를 사용한 선형 회귀

![img](https://velog.velcdn.com/images/jochedda/post/60bfb9c8-00bc-489e-8e9e-da2f6f1588d0/image.png)

→ 라쏘 규제를 사용한 다항 회귀

a = 1e-07 의 그래프는 3차 방정식 처럼 보인다.
**- ✅ 라쏘 회귀는 자동으로 특성 선택을 하고 희소 모델을 만든다.
**

### 🤔 라쏘 회귀(Lasso Regression) 과 릿지 회귀(Ridge Regression) 의 차이점을 알아보자 ‼️

![img](https://velog.velcdn.com/images/jochedda/post/d653c267-686d-4fb4-bb6a-9833a35b4613/image.png)

🟨 **라쏘 회귀 :** 왼쪽 위 그래프가 **라쏘의 손실**을 나타낸다 . *θ*1=2,*θ*2=0.5 값에서 경사하강법을 실시하면 *θ*2 가 먼저 0에 도달한다. 오른쪽 위 그래프는 **라쏘의 손실 함수**를 나타낸다. 여기서도 ***θ\*2=0으로 빠르게 줄어들고 그 다음 축을 따라 진동하며 전역 최적점에 도달한다**.

🟨 **릿지 회귀:** 왼쪽 아래 그래프는 **릿지의 손실**을 나타낸다. 원점에 가까울수록 손실이 줄어드는 것을 확인할 수 있다. 오른쪽 아래 그래프는 **릿지의 손실 함수**를 나타낸다. ***α\* 를 증가시킬수록 최적의 파라미터가 원점에 더 가까워진다. (진동이 없음)**

✔️ 라쏘는 `제약 조건이 절댓값 이기 때문에 마름모 꼴`로 나타난다. 라쏘 회귀의 경우 최적값은 모서리 부분에서 나타날 확률이 릿지에 비해 높기 때문에 몇몇 **`유의미하지 않은 변수들에 대해 계수를 0에 가깝게 (또는 0) 으로 추정해 feature selection의 효과`** 를 가져온다. `라쏘는 파라미터의 크기에 관계 없이 같은 수준의 reularizaton 을 적용하기 때문에 작은 값의 파라미터를 0으로 만들어 해당 변수를 모델에서 삭제하고, 따라서 모델을 단순하게 만들어주고 해석을 용이하게 한다.`

✔️ 하지만 릿지의 경우 어느정도 **`상관성을 가지는 변수들에 대해 pulling 이 되는 효과를 보여줘서 변수 선택 보다는 상관성이 있는 변수들에 대해 적절한 가중치 배분을 하게 된다.`** 따라서 릿지의 경우 PCA 와 관련성이 있다.

### 🟠 Ridge 와 Lasso

| Ridge                                           | Lasso                                                        |
| ----------------------------------------------- | ------------------------------------------------------------ |
| L2 - norm                                       | L1 - norm                                                    |
| 변수 선택 불가능                                | 변수 선택 가능                                               |
| Closed form solution 존재 ( 미분으로 구함 )     | Closed form solution이 존재하지 않음(numerical optimization 이용) |
| 변수 간 상관관계가 높은 상황에서 좋은 예측 성능 | 변수 간 상관관계가 높을 때 Ridge에 비해 에측 성능이 떨어짐   |
| 크기가 큰 변수를 우선적으로 줄이는 경향이 있음  |                                                              |

### 🟠 사이킷런을 이용한 Lasso 코드

```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha = 0.1)
lasso_reg.fit(X,y)
lasso_reg.predict([[1.5]])
```

💡 Lasso 대신 **SGDRegressor(penalty = “l1”)** 을 사용할 수도 있다.



## **사전 배경 이해** 

통계 데이터 분석에 있어서 모델의 단순화는 모델 일반화를 위해 매우 중요하다. 

일반적인 다중선형회귀에서는



![img](https://blog.kakaocdn.net/dn/cfQSTg/btqUGVc1r8S/TS2ZRQLyXfj7Qq144C6go1/img.png)



1. 독립변수 갯수가 표본크기에 비해서 지나치게 많을 경우 제대로된 성능을 발휘하기 어렵다.

불필요한 회귀계수는 모델의 예측성능을 저하시키기 떄문

2. 많은 독립변수가 존재할 경우 다중공선성으로 인해 회귀계수의 영향력이 과도하게 높게 측정될 수 있다.

 

## **패널티 회귀분석 (Penalized Regression)**

: 너무 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 기존 선형회귀의 과적합을 방지시키는 방법이다.

ㄴ모델 성능에 크게 기여하지 못하는 변수의 영향력을 제거하거나 축소킨다.

ㄴ일반적인 선형회귀는 MSE(잔차제곱합이 최소)으로 계산하지만 패널티 회귀분석에는 제곱합에 패널티 항이 붙어 

 제곱합과 패널티항의 합이 최소가 되는 회귀계수를 추정한다.

ㄴ대표적인 패널티 회귀분석으로는 릿지회귀분석, 라소회귀분석, 일래스틱 회귀분석이 있다.

### **릿지회귀분석 (Ridge Regression)**

: 모델의 설명력에 기여하지 못하는 독립변수의 회귀계수 크기를 0에 근접하도록 축소시킨다. (작아져도 0은 되지 않음)

 



![img](https://blog.kakaocdn.net/dn/WFVUI/btqUJXPbNPW/v9V0kjc7qEPyFWOOaKf3q0/img.png)



L2-norm 이라고 부르는 페널티항을 통해 일반 선형회귀 모델에 페널티를 부과하는 방법으로

회귀계수를 축소시킨다.

 

**식 이해**

1) 베타

: 각 회귀계수 ~ 이 베타(각 회귀계수)를 제곱한뒤 더한 값을 감가값과 곱하는 것이다. 

2) 하이퍼파라미터 튜닝

: 감마 값이 커짐에 따라 회귀계수는 작아질 수 밖에 없다.

 or 감마 값이 작아짐에 따라 회귀계수는 원래 선형회귀 처럼 계산하게 되는 것

 

**주의사항**

: 릿지 회귀분석은 독립변수 척도에 크게 영향을 받는다. 표준적인 선형회귀 분석과 더욱 영향을 받는 알고리즘이므로,

릿지 회귀를 적용하기 전에 모든 독립변수의 척도가 동일하도록 표준화하는 작업이 선행되어야 한다.

 

**릿지 회귀 특징**

\> 관측갯수 보다 독립변수가 상대적으로 많을 경우, 일반 표준 선형회귀보다 릿지회귀가 더욱 우수한 성능을 발휘

\> 릿지 회귀분석은 회귀계수를 0에 가깝도록 축소시키지만 어떤 회귀계수도 0으로 만들지는 않는다.

ㄴ즉, 원래 데이터셋에 있는 모든 독립변수가 릿지회귀에서도 그대로 적용된다는 것을 말한다.

ㄴ따라서 단계선택법을 통해 통계적으로 유의한 독립변수만을 유지할 수 있는 선형회귀와는 달리,

 릿지 회귀분석은 회귀모델의 모든 독립변수를 그대로 보존하고 있다. ( 또 반대로 말하면, 변수선택법 제공 불가)

### **라쏘 회귀분석 (Lasso Regression)**

: 라쏘회귀모델은 릿지회귀모델과 다르게 설명력에 기여하지 못하는 독립변수의 회귀계수를 0으로 만드는 방법이다.

ㄴ 릿지회귀와 달리 회귀계수가 0이 될 수 있기 때문에 변수선택을 통해 더 간단한 모델을 만들 수 있다. 



![img](https://blog.kakaocdn.net/dn/bjtX4M/btqUJXhlEWS/DMbakLEgycI5PTCNKgDlyk/img.png)식1_베타(회귀계수 보기 편하게 w로 표시)

![img](https://blog.kakaocdn.net/dn/cmK8uS/btqUM83vlbP/KF5KneCCerkpRktqWyjMj1/img.png)



라쏘회귀에서는 L1-norm 패널티항으로 회귀모델에 패널티를 부과함으로써 회귀계수를 축소시킨다.

 

**식 이해** 

베타 : 각 회귀계수~ 각 회귀계수를 절대값 하여 계산하여 추정한다.

감마 : 릿지회귀와 마찬가지로 패널티의 크기를 결정하는 하이퍼파라미터 

 

**릿지와의 차이**

: 라쏘회귀모델에서 패널티항은 모델에 대한 기여도가 낮은 회귀계수를 0으로 만들 수 있다. 

ㄴ이는 해당 변수가 모델에서 제거됨을 의미한다.

ㄴ따라서 라쏘회귀모델을 이용하면 변수선택을 통해 설명력이 우수한 독립변수만으로 모델을 생성해 단순화시킬 수 있다. (모델이 더 간명해짐에 따라 해석가능한 모델을 생성한다는 장점을 갖는다)

 

\> 단, 이 특장점이 라쏘 회귀모델이 릿지 회귀모델보다 우수하다는 것을 의미하지는 않는다. 

ㄴ 라쏘 회귀분석에서는 일부 독립변수를 제거할 수 있기 떄문에, 일부 독립변수의 설명력이 크고, 나머지 독립변수의 설명변수가 설명력이 낮을 때 우수한 성능을 보여준다. 

ㄴ 반면, 릿지회귀는 많은 독립변수의 선형결합으로 모델을 생성하기 때문에 이 독립변수들의 설명력이 서로 차이가 크게 나지 않을수록 우수한 성능을 보여준다. 

ㄴ 결론적으로 모두 다 사용해서 평가해야 한다. 

### **일래스틱회귀분석**

: 릿지와 라쏘의 결합이다. 즉 L1-norm 과 L2-norm을 모두 이용하여 패널티를 부과하여 회귀모델을 생성한다. 

ㄴ 릿지 회귀모델처럼 모델을 충분하게 설명하지 못하는 일부 독립변수의 회귀계수 크기를 축소할 수 있고,

  랏쏘 회귀모델처럼 모델을 충분하게 설명하기 못하는 일부 독립변수으 회귀계수를 강제로 0으로 할당하여

  특정 독립변수를 모델에서 제거할 수 있다. 



![img](https://blog.kakaocdn.net/dn/blUoUU/btqUJYAxoTC/0gpF2XSdtMNg8b4N29HM90/img.png)



**식 이해**

알파 : 알파가 작을 수록 릿지 회귀모델(L1-norm)이 더 강하게 작용하고, 클수록 라쏘 회귀모델(L2-norm)이 더 크게 작용

감마 : 마찬가지로 패널티항(L1-norm, L2-norm)의 파라미터 튜닝



source : https://velog.io/@jochedda/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EB%9D%BC%EC%8F%98-%ED%9A%8C%EA%B7%80Lasso-Regression,

https://challenge.tistory.com/30