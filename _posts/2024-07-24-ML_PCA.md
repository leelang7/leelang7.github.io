## ***\*1. K-Means 클러스터링\****

K-Means 클러스터링은 가장 간단하고 널리 사용되는 클러스터링 알고리즘입니다.

K-Means 클러스터링은 데이터의 영역을 대표하는 Cluster-center(중심점)을 찾아가며 데이터를 클러스터에 할당합니다.

이번 실습에서는 사이킷런에 구현된 K-Means 클러스터링 모델을 불러와 클러스터링 결과를 시각화를 통해 확인해 보겠습니다.

------

***\*K-Means 클러스터링을 위한 사이킷런 함수/라이브러리\****

- `from sklearn.cluster import KMeans`: K-Means 클러스터링을 위한 모델을 불러옵니다.
- `model=KMeans(init, n_clusters, random_state)` : K-Means 클러스터링 모델 `kmeans`을 정의합니다.
- - `init` : 중심점 초기화 방법 설정(`'random'` 으로 설정할 경우 랜덤으로 중심점을 초기화함)
  - `n_clusters` : 군집의 개수, 즉 군집 중심점의 개수
  - `random_state` : 일관된 결과 확인을 위한 설정값
- `model.fit(X)` : 데이터 X에 대해 `kmeans`모델을 활용하여 클러스터링을 수행합니다.
- `model.labels_` : 각 데이터가 속한 군집 중심점 label(클러스터링 결과)를 반환합니다.

## ***\*실습\****

1. K-means 클러스터링을 수행하는 함수 `k_means_clus()` 를 구현합니다.
2. - K-Means 객체를 불러옵니다.`n_cluster`는 ***\*3\****, `init`는 **`\**'random'\**`**, `random_state` 는 ***\*100\****으로 설정합니다.
   - 클러스터링은 ***\*정답이 없는 데이터를 사용\****하기 때문에 `drop()` 함수를 사용하여`'target'` ***\*컬럼을 제거한 데이터\****를 학습시켜줍니다.
   - 군집화 결과 즉, 각 데이터가 속한 군집 중심점들의 label을 iris 데이터 프레임에 `'cluster'` 컬럼으로 추가합니다.
3. 실행 버튼을 눌러 `iris_result`를 보고 군집화가 어떻게 이루어졌는지 확인해 보세요.

[실행 결과]

![image_output.png](https://cdn-api.elice.io/api-attachment/attachment/be9c5e81c1f741d7ba665aeb78aec232/image_output.png)



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## ***\*2. Gaussian Mixture Model (GMM)\****

Gaussian Mixture Model(GMM)을 이용한 클러스터링은 데이터가 여러 개의 정규 분포를 가지고 있다고 가정한 후 클러스터링을 진행하는 방식입니다.

여러 개의 데이터 세트가 있다면 이를 구성하는 여러 개의 정규 분포 곡선을 추출하고, 개별 데이터가 이 중 어떤 정규 분포에 속하는지 결정합니다.

또한, GMM은 K-means 클러스터링과 달리 군집 중심점(clustering center)가 없으므로 군집의 개수(K)를 정해주면 됩니다.

이번 실습에서는 동일한 iris 데이터를 가지고 GMM을 이용한 클러스터링을 수행하고, 결과를 시각화하여 확인해 보겠습니다.

------

***\*GMM을 위한 사이킷런 함수/라이브러리\****

- `from sklearn.mixture import GaussianMixture` : 사이킷런에 구현되어 있는 GMM 모델을 불러옵니다.
- `model=GaussianMixture(n_components, random_state)`: GMM모델을 정의합니다.
- - `n_components` : 군집의 개수
  - `random_state` : 일관된 결과 확인을 위한 설정값
- `model.fit(X)` : X에 대해 모델을 활용하여 클러스터링을 수행합니다.
- `model.predict(X)` : X에 대해 모델을 활용한 클러스터링 결과를 반환합니다.

## ***\*실습\**** 

1. GMM 클러스터링을 수행하는 함수`gmm_clus()`를 구현합니다.
2. - GMM 객체를 불러옵니다. `n_components`는 ***\*3\****, 일관된 결과 확인을 위해 `random_state`를 ***\*100\****으로 설정합니다.
   - GMM 클러스터링을 수행합니다. 클러스터링은 ***\*정답이 없는 데이터를 사용\****하기 때문에 `drop()` 함수를 이용해 `'target'` 컬럼을 제거한 데이터를 학습시켜줍니다.
   - 군집화 결과를 iris 데이터 프레임에 `'cluster'`컬럼으로 추가합니다.
3. 실행 버튼을 눌러 `iris_result`를 보고 군집화가 어떻게 이루어졌는지 확인해보세요.

[실행 결과]

![image_output (1).png](https://cdn-api.elice.io/api-attachment/attachment/23ec9a2163f1447d93587177e99114c0/image_output%20%281%29.png)



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## ***\*3. K-Means VS GMM\****

K-Means 클러스터링은 군집화 범위가 원형입니다. 그러므로 데이터 세트의 분포가 원형에 가까울수록 효율이 높아집니다.

그러나 실제 데이터의 분포가 원형인 경우는 적습니다. 데이터의 분포가 만약 타원의 형태로 길쭉하게 늘어져 있다면 K-means 클러스터링은 해당 데이터에 대하여 제대로 군집화를 진행하기 어렵습니다.

이런 경우에는 데이터의 분포 방향에 따라 군집화를 진행하는 GMM 클러스터링을 사용하면 군집화 성능을 높일 수 있습니다.

### ***\*임의의 데이터 생성\****

클러스터링 알고리즘을 테스트하기 위한 데이터 생성기를 사용해 보겠습니다.

대표적으로 `make_blobs()` 를 사용합니다.
이를 이용해 타원형 분포를 가진 데이터를 만들어냅니다.

------

***\*타원형 분포 데이터 생성을 위한 사이킷런 함수/라이브러리\****

- `make_blobs(n_samples, n_features, centers, cluster_std, random_state)`: 클러스터링 가상의 데이터를 생성합니다.
- - `n_samples`: 생성할 데이터의 총 개수
  - `n_features`: 데이터의 변수 개수, 시각화를 위해 보통 2개로 설정
  - `centers`: 군집의 개수
  - `cluster_std`: 데이터의 표준편차
  - `random_state`: 일관된 결과를 위한 설정값

## ***\*실습\****

1. 지시 사항과 동일한 타원형 분포의 데이터를 생성합니다.
2. - `n_samples`는 ***\*300개\****, `n_features`는 ***\*2개\****, `centers`는 ***\*3개\****, `cluster_std`는 ***\*0.8\****, `random_state`는 ***\*0\****으로 설정합니다.
3. K-Means 클러스터링을 수행하여 클러스터링 결과를 데이터 프레임 내에 저장하는 함수 `K_means()`를 완성합니다.

- - 데이터 X_aniso에 대한 K-Means클러스터링을 수행합니다. `init`은 `'random'`, `n_clusters`는 ***\*3\****, `random_state`는 ***\*0\****으로 설정합니다.
  - `kmeans_label`변수에클러스터링 결과를 저장합니다.

1. GMM 클러스터링을 수행하여 클러스터링 결과를 데이터 프레임 내에 저장하는 함수 `GMM()`를 완성합니다.

- - 데이터 X_aniso에 대한 GMM 클러스터링을 수행합니다. `n_components`는 ***\*3\****개, `random_state`는 ***\*0\****으로 설정합니다.
  - `gmm_label`변수에 클러스터링 결과를 저장합니다.

1. 실행 버튼을 눌러 타원형 분포에서의 K-Means 클러스터링 결과와 GMM 클러스터링 결과를 확인해보세요.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## ***\*4. 주성분 분석(PCA)\**** 

PCA를 사용하면 가장 덜 중요한 축들은 제거되고, 가장 중요한 상위 축만 남겨집니다.

이렇게 데이터 내에서 영향을 덜 주는 변수들이 사라지면, 점들 간에 가장 중요한 관계를 맺는 차원들만을 남길 수 있습니다.

예를 들어, 변수를 반으로 줄였을 때 대부분의 점들이 잘 보존된다면, 적은 정보로 값을 그대로 표현할 수 있어 데이터를 사용하기에 훨씬 효율적일 것입니다.

이번 실습에서는 2차원으로 고정한 wine data를 주성분이 있는 PCA를 사용하여 1차원으로 축소할 때 결과를 확인해 보도록 하겠습니다.

------

***\*PCA를 위한 사이킷런 함수/라이브러리\****

- `from sklearn.decomposition import PCA` : 사이킷런에 구현되어 있는 주성분 분석(PCA) 모델을 불러옵니다.
- `pca=PCA(n_components)`: n_components 개수로 데이터의 차원을 축소하는 주성분 분석 모델 `pca`를 정의합니다.
- `pca.fit(X)` : X에 대해 `pca`모델을 활용하여 차원 축소를 진행합니다.
- `pca.transform(X)`: X를 `pca`모델을 활용해 설정한 `n_components`로 차원을 축소시킨 결과 데이터를 반환합니다.

## ***\*실습\****

1. 사이킷런에 저장된 데이터를 불러오고, 2개의 변수만을 가질 수 있도록 고정하여 반환하는 함수`load_data()`를 구현합니다.
2. - `load_wine()`을 활용해 사이킷런에 저장된 와인 데이터를 불러옵니다.데이터는 `(X, y)`형태로 불러와야 합니다. (return_X_y를 `True`로 지정해야 합니다)
   - `column_start`로 지정된 특정 column으로부터 연속되는 2개의 변수를 `X`에 저장합니다.
3. 주성분 분석(PCA)을 수행하여 2차원 데이터를 1차원으로 축소하는 함수 `pca_data()`를 완성합니다.
4. - PCA의 `n_components`를 ***\*1\****로 지정하여 pca를 정의합니다.
   - 주성분 분석을 수행합니다.
   - `X_pca`값을 추출합니다.
5. 실행 버튼을 눌러 출력된 original shape와 transformed shape를 비교해 보고, 이미지에서 주황색 선을 잘 살펴보세요.

[실행 결과]

![image_output (2).png](https://cdn-api.elice.io/api-attachment/attachment/70e4fa1e6e50450499a5c4c38ad1e8d8/image_output%20%282%29.png)



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## ***\*5. t-SNE\****

t-SNE 알고리즘은 데이터 시각화를 위해 주로 활용되는 차원 축소 알고리즘입니다.

데이터 간의 거리에 상관없이 함축적으로 표현되는 PCA와는 달리 t-SNE는 차원 축소 이후에도 가까운 데이터는 가깝게, 멀리 있는 데이터는 멀게 표현할 수 있습니다. 따라서 대용량 데이터의 시각화 즉, 구별 가능한 시각화를 위해 활용할 수 있습니다.

이번 실습에서는 주성분 분석(PCA)에서 사용한 와인 데이터를 활용하여 t-SNE 알고리즘을 사용하는 방법을 학습해보도록 하겠습니다.

------

***\*t-SNE를 위한 사이킷런 함수/라이브러리\****

- `from sklearn.manifold import TSNE` : 사이킷런에 구현되어 있는 tsne 모델을 불러옵니다.
- `tsne=TSNE(n_components)`: `n_components`개수로 데이터의 차원을 축소하는 t-SNE모델 `tsne`를 정의합니다.
- `tsne.fit_transform(X)` : X를 `tsne`모델을 활용해 설정한 n_components로 차원을 축소시킨 결과 데이터를 반환합니다.

## ***\*실습\**** 

1. 사이킷런에 저장된 데이터를 불러오고, 2개의 변수만을 가질 수 있도록 고정하여 반환하는 함수`load_data()`를 구현합니다.
2. - `load_wine()`을 활용해 사이킷런에 저장된 와인 데이터를 불러옵니다. 데이터는 `(X, y)` 형태로 불러와야 합니다. (return_X_y를 `True`로 지정해야 합니다)
   - `column_start`로 지정된 특정 column으로부터연속되는 2개의 변수를 `X`에 저장합니다.
3. t-SNE를 활용하여 2차원 데이터를 1차원으로 축소하는 함수 `tsne_data()`를 완성합니다.
4. - t-SNE의 `n_components`를 ***\*1\****로 지정하여 `tsne`를 정의합니다.
   - `tsne`를 활용하여 차원 축소를 진행한 후, 차원이 축소된 데이터 `X_tsne`를 추출합니다.
5. 출력된 original shape와 transformed shape와 원본 데이터 X값과 차원 축소 이후 데이터 값을 비교하며 확인해봅니다.



------

## Tips!

- 지시사항에 따라 None값을 채웁니다.
- None값이 아닌 주어진 값을 변경하면 오류가 발생할 수 있습니다.



## ***\*6. 군집화 모델의 군집 수 결정\****

학습 과정에서는 Inertia값을 이용해서 학습 과정동안 Inertia값이 줄어드는 것을 확인했습니다. 하지만, K 가 증가할 수록 Inertia는 줄어드는 양상을 보이기 때문에, 모델간의 비교가 가능한 silhouette coefficient를 사용해 군집화 모델 사이의 성능을 비교합니다.

이번 실습에선 모델의 하이퍼 파라미터들을 스스로 변경해보며 데이터 분포에 가장 최적인 파라미터를 찾는 ***\*모델 평가\**** 단계를 진행해봅시다.

우리의 목표는 최적의 군집 수 파라미터, K를 찾아서 K-평균 군집화 모델에 대한 silhouette 값을 ***\*0.46 이상\****으로 높이는 것입니다.

\---------------------------------------------

### ***\*코드 구성\****

`preprocess.py` : data 폴더 안의 iris.csv 파일을 읽고 데이터를 저장합니다.

`main.py` : 각 모델의 군집화 결과에 대한 silhouette 값을 계산하여 출력합니다.

- `silhouette_calculator()`: 데이터 셋과 군집 할당 정보를 인수로 받아 silhouette 점수를 계산합니다.
- `your_choice()`: 여러분이 답으로 제출하고 싶은 군집의 개수로 '?' 부분에 넣으면 해당 군집 개수로 군집화 모델을 생성해 출력합니다.

## **실습**

##### 1. `main.py` :

- ```
  silhouette_calculator()
  ```

   

  함수를 완성합니다.

  - 인수로 X (numpy ndarray), cluster ( 학습된 모델의 cluster dictionary) 를 받습니다.
  - `scikit-learn`의 metric 서브 패키지의 [`silhouette_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)함수를 사용할 수 있습니다.

  `silhouette_score(X, labels, *, metric='euclidean', ...)`
  \- `X`: numpy ndarray값을 받습니다. shape = (n_samples_a, n_features)입니다.
  \- `labels`: `X`의 각 행, 𝑖*i* 번째 데이터 포인트에 대한 군집의 label에 대한 리스트를 받습니다.
  \- `metric`: silhouette 값을 계산하기 위해 개체간 거리를 계산할 때, 거리의 metric으로 무엇을 사용할지 받습니다. 기본값은 ‘euclidean’입니다.

- 구현한 `silhouette_calculator()` 함수를 통해 silhouette값을 계산합니다.

- ```
  KMeans()
  ```

   

  모델의 모수

   

  𝐾*K*

  를 바꿔보며 실습을 진행합니다.

  - 최종적으론 silhouette 값을 **0.46 이상**으로 만드는 𝐾*K*을 찾아봅니다.
  - `your_choice()`: 여러분이 답으로 제출하고 싶은 군집의 개수로 ‘?’ 부분에 넣은 후 제출하세요.



## ***\*7. PCA 성분의 개수 고르기\****

PCA 모델의 학습을 통해 K 차원으로 차원 축소를 하지만, 몇 개의 차원으로 축소하는 것이 차원을 축소하면서도 동시에 적절한 수준의 정보를 유지할 수 있을지 결정해야 합니다. 이를 위해서 차원이 축소되면서 기존 분산의 몇 퍼센트의 분산이 유실되는지를 기준으로 정할 수 있습니다.

이번 실습에선 모델의 하이퍼 파라미터들을 스스로 변경해보며 데이터 분포에 가장 최적인 파라미터를 찾는 ***\*모델 평가\**** 단계를 진행해봅시다.

우리의 목표는 1에서 *M* 까지 가능한 성분의 개수 중에서, 최적의 차원 축소 전략을 짜는 것입니다.

최적의 차원을 정하기 위해서, 다음 ***\*분산유지율\****을 최소로하는 가장 적은 차원으로 정하기로 합니다.



*R*=∑*m*=1*M**λ**m*∑*k*=1*K**λ**k*



여기서 *R*은 기존 *M* 차원에서 *K* 차원으로의 PCA를 통한 변환으로, 유실되지 않고 유지되는 분산의 비율을 의미합니다.

PCA 모델에 대한 `n_components` 파라미터를 찾아서 ***\*전체 분산의 90% 이상\****을 남길 수 있도록 차원축소 하는 것입니다.

\---------------------------------------------

### ***\*코드 구성\****

`preprocess.py` : data 폴더 안의 iris.csv 파일을 읽고 데이터를 저장합니다.

`main.py` : 각 모델의 차원 축소 후의 분산 값을 계산하여 출력합니다.

- `variance_calculator()`: 학습된 모델을 인수로 받아 분산 값을 계산합니다.
- `your_choice()`: 여러분이 답으로 제출하고 싶은 성분의 개수를 '?' 부분에 넣으면 해당 성분의 개수로 차원축소를 진행하는 모델을 생성해 출력합니다.

## **실습**

##### 1. `main.py` :

- ```
  variance_calculator()
  ```

   

  함수를 완성합니다.

  - 인수로 model ( 학습된 모델)을 받습니다.
  - 원본의 분산을 구합니다.
  - 차원축소된 벡터공간에서의 분산을 계산합니다.
  - 차원축소된 벡터공간에서의 분산을 원본의 분산으로 나눠 분산 유지율을 계산해 반환합니다.

- ```
  main()
  ```

  함수 완성

  - ```
    PCA()
    ```

     

    모델의 모수

     

    ```
    n_components
    ```

    를 바꿔보며 실습을 진행합니다.

    - 최종적으론 분산의 보존양이 기존의 **95% 이상**인 최소의 `n_components`를 찾아봅니다.
    - `your_choice()`: 여러분이 답으로 제출하고 싶은 군집의 개수로 ‘?’ 부분에 넣은 후 제출하세요.

  - 학습된 모델과 구현된 `variance_calculator()`를 사용하여, 원본 대비 차원 축소후의 분산의 비율을 계산합니다.